{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install mdptoolbox-hiive\n! pip install gym\n! pip install pymdptoolbox","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport hiive.mdptoolbox \nimport hiive.mdptoolbox.mdp\nimport hiive.mdptoolbox.example\nimport mdptoolbox, mdptoolbox.example\nimport gym\nimport matplotlib.pyplot as plt\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# P, R = hiive.mdptoolbox.example.forest(S=2000, p=.01)\n# pim_test = hiive.mdptoolbox.mdp.ValueIteration(P, R, 0.999999, epsilon=0.01, max_iter=1000)\n# pim_test.run()\n# pim_test.run_stats\n# forest_pi_mdp = mdptoolbox.mdp.PolicyIterationModified(P, R, 0.99999, epsilon=0.01, max_iter=10**6, skip_check=True)\n# forest_pi_mdp.run()\n# forest_pi_mdp.policy\n# print(\"forest_pi_mdp.policy\", forest_pi_mdp.policy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_simple_data(x_var, y_var, x_label, y_label, title, figure_size=(4,3)):\n    plt.rcParams[\"figure.figsize\"] = figure_size\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.plot(x_var, y_var, 'o-')\n    plt.show()\n\ndef plot_data_legend(x_vars, x_label, all_y_vars, y_var_labels, y_label, title, y_bounds=None):\n    colors = ['red','orange','black','green','blue','violet']\n    plt.rcParams[\"figure.figsize\"] = (4,3)\n\n    i = 0\n    for y_var in all_y_vars:\n#         if i == 2: # don't plot when i = 1 for cv\n#             x_vars = x_vars[1:]\n        plt.plot(x_vars, y_var, 'o-', color=colors[i % 6], label=y_var_labels[i])\n        i += 1\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    if y_bounds != None:\n        plt.ylim(y_bounds)\n    leg = plt.legend()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_time_array(run_stats, variables):\n    cumulative_sum = 0\n    times = []\n    output_dict = {v:[] for v in variables}\n    output_dict[\"times\"] = times\n    for result in run_stats:\n        times.append(result[\"Time\"])\n        for v in result:\n            if v in variables:\n                output_dict[v].append(result[v])\n    return output_dict\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P, R = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\nst = time.time()\nfm_q_mdp = hiive.mdptoolbox.mdp.QLearning(P, R, 0.999, epsilon=0.1,epsilon_decay=0.95, n_iter=1000000, alpha=0.95, skip_check=True)\nfm_q_mdp.run()\nend = time.time()\nend-st\nfm_q_mdp.policy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fm_q_curated_results = make_time_array(fm_q_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\nnum_iters = len(fm_q_curated_results[\"Mean V\"])\nplot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"Q-Learning Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"Q-Learning Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"Q-Learning Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compose_discounts(significant_digits):\n    prev_discount = 0\n    discounts = []\n    for i in range(1,significant_digits + 1):\n        discounts.append(round(prev_discount + 9*(10**-i),i))\n        prev_discount = discounts[-1]\n    return discounts\n\ndef run_forest(solver, states, discounts, epsilons, probability=0.1, max_iter=10):\n    experiments = [] #num states, probability, discount, time, iterations, policy\n    for s in states:\n        for e in epsilons:\n            for d in discounts:\n                entry = {}\n                P, R = hiive.mdptoolbox.example.forest(S=s, p=probability)\n                #start_time = time.time()\n                args = {\"transitions\":P, \"reward\":R, \"gamma\":d, \"epsilon\":e, \"max_iter\":max_iter, \"skip_check\":True}\n                mdp = solver(args)\n                mdp.run()\n                #end_time = time.time()\n                entry[\"time\"] = mdp.time\n                entry[\"iterations\"] = mdp.iter\n                entry[\"policy\"] = mdp.policy\n                entry[\"num_states\"] = s\n                entry[\"probability\"] = probability\n                entry[\"discount\"] = d\n                entry[\"epsilon\"] = e\n                entry[\"run_stats\"] = mdp.run_stats\n                experiments.append(entry)\n    return experiments\n       \n\n#number of states by time/iterations\n#discount over time/iterations\n#p over number of 1's in policy\n#epsilon over quality\n\n#TODO quality","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states = [10**s for s in range(1,4)]\ndiscounts = compose_discounts(3)\ndiscounts = [0.999999,0.9999999]\nepsilons = [0.01, 0.005, 0.001]\n\nfm_policy_iteration = lambda dict_args: hiive.mdptoolbox.mdp.PolicyIterationModified(**dict_args)\nfm_policy_iteration_results = run_forest(fm_policy_iteration, states, discounts, epsilons)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states = [10**s for s in range(2,4)]\ndiscounts = compose_discounts(5)\nepsilons = [0.01, 0.005, 0.001]\n\n\nfm_value_iteration = lambda dict_args: hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_value_iteration_results = run_forest(fm_value_iteration, states, discounts, epsilons)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fm_value_iteration_results\ndef print_training_results(results):\n    for result in fm_value_iteration_results:\n        print(\"\\nNew result #################\")\n        for key in result:\n            if key != \"policy\":\n                print(\"{0}: {1}\".format(key,result[key]))\ndef collect_training_results(results, keys, to_print=True):\n    output_dict = {key:[] for key in keys}\n    for result in results:\n        if to_print: print(\"\\nNew result #################\")\n        for key in result:\n            if key in keys:\n                if to_print: print(\"{0}: {1}\".format(key,result[key]))\n                output_dict[key].append(result[key])\n    return output_dict\n                \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states = [k*10**pwr for pwr in range(2,3) for k in range(1,10)]\nstates += [1000]\nstates += [1000 + s for s in (states) ]\n\ndiscounts = [0.9999999]\nepsilons = [0.1]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfm_value_iteration = lambda dict_args: hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_value_iteration_results = run_forest(fm_value_iteration, states, discounts, epsilons)  \n#print_training_results(fm_value_iteration_results)\nfm_vi_time_num_states = collect_training_results(fm_value_iteration_results, [\"time\", \"num_states\"], to_print=False)\nfm_vi_iters_num_states = collect_training_results(fm_value_iteration_results, [\"iterations\", \"num_states\"], to_print=False)\nplot_simple_data(fm_vi_time_num_states[\"num_states\"], fm_vi_time_num_states[\"time\"], \"num_states\", \"time\", \"Forest Mgmt Performance with Value Iteration\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fm_policy_iteration = lambda dict_args: hiive.mdptoolbox.mdp.PolicyIterationModified(**dict_args)\nfm_policy_iteration_results = run_forest(fm_policy_iteration, states, discounts, epsilons)\nfm_pi_time_num_states = collect_training_results(fm_policy_iteration_results, [\"time\", \"num_states\"], to_print=False)\nfm_pi_iters_num_states = collect_training_results(fm_policy_iteration_results, [\"iterations\", \"num_states\"], to_print=False)\nplot_simple_data(fm_pi_time_num_states[\"num_states\"], fm_pi_time_num_states[\"time\"], \"num_states\", \"time\", \"Forest Mgmt Performance with Policy Iteration\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states = [2000]\ndiscounts = [0.99]\nepsilons = [0.75,0.5,0.25,0.1,0.01, 0.001]\nfm_value_iteration = lambda dict_args: hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_value_iteration_results = run_forest(fm_value_iteration, states, discounts, epsilons, probability=0.0001, max_iter=10**2)  \n#print_training_results(fm_value_iteration_results)\nfm_vi_time_num_states = collect_training_results(fm_value_iteration_results, [\"time\", \"epsilon\", \"iterations\"], to_print=False)\nplot_simple_data(fm_vi_time_num_states[\"epsilon\"], fm_vi_time_num_states[\"time\"], \"epsilon\", \"time\", \"Forest Mgmt Value Iteration Training Time over Epsilon\")\nplot_simple_data(fm_vi_time_num_states[\"epsilon\"], fm_vi_time_num_states[\"iterations\"], \"epsilon\", \"iterations\", \"Forest Mgmt Value Iteration Iterations over Epsilon\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states = [2000]\ndiscounts = [0.99]\nepsilons = [0.75,0.5,0.25,0.1,0.01, 0.001]\nfm_policy_iteration = lambda dict_args: hiive.mdptoolbox.mdp.PolicyIterationModified(**dict_args)\nfm_policy_iteration_results = run_forest(fm_policy_iteration, states, discounts, epsilons, probability=0.0001, max_iter=10**2)\nfm_pi_time_num_states = collect_training_results(fm_policy_iteration_results, [\"time\", \"epsilon\"], to_print=False)\nfm_pi_iters_num_states = collect_training_results(fm_policy_iteration_results, [\"iterations\", \"epsilon\"], to_print=False)\nplot_simple_data(fm_pi_time_num_states[\"epsilon\"], fm_pi_time_num_states[\"time\"], \"epsilon\", \"time\", \"Forest Mgmt Performance with Policy Iteration\")\nplot_simple_data(fm_pi_iters_num_states[\"epsilon\"], fm_pi_iters_num_states[\"iterations\"], \"epsilon\", \"iterations\", \"Forest Mgmt Policy Iteration Iterations over Epsilon\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_pi_fm, R_pi_fm = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\ndict_args = {\"transitions\":P_pi_fm, \"reward\":R_pi_fm, \"gamma\":0.9999, \"epsilon\":0.1, \"max_iter\":10**3}\nfm_pi_mdp = hiive.mdptoolbox.mdp.PolicyIteration(P_pi_fm, R_pi_fm, 0.999, max_iter = 5*10**2, skip_check=True)\nfm_pi_mdp.run()\nprint(fm_pi_mdp)\nfm_pi_mdp_curated_results = make_time_array(fm_pi_mdp.run_stats, [\"Mean V\", \"Max V\"])\nnum_iters = len(fm_pi_mdp_curated_results[\"Mean V\"])\nplot_simple_data([i for i in range(num_iters)], fm_pi_mdp_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"PI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data([i for i in range(num_iters)], fm_pi_mdp_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"PI Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data([i for i in range(num_iters)], fm_pi_mdp_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"PI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_pi_fm, R_pi_fm = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\ndict_args = {\"transitions\":P_pi_fm, \"reward\":R_pi_fm, \"gamma\":0.999,\"epsilon\":10**(-50), \"max_iter\":10**5, \"skip_check\":True}\nfm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_vi_mdp.run()\nprint(fm_vi_mdp)\nfm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\"])\nnum_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\nprint(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\nplot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"max mean v pi\", max(fm_pi_mdp_curated_results[\"Mean V\"]))\nneq = []\nfor i in range(len(fm_vi_mdp.policy)):\n    if fm_vi_mdp.policy[i] != fm_pi_mdp.policy[i]:\n        neq.append(i)\nlen(neq)\nsum(fm_vi_mdp.policy) < sum(fm_pi_mdp.policy)\nsum(fm_pi_mdp.policy)\nneq\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_pi_fm, R_pi_fm = hiive.mdptoolbox.example.forest(S=2000, p=0.01, r1=1000000)\ndict_args = {\"transitions\":P_pi_fm, \"reward\":R_pi_fm, \"gamma\":0.999,\"epsilon\":10**(-10), \"max_iter\":10**5, \"skip_check\":True}\nfm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_vi_mdp.run()\nprint(fm_vi_mdp)\nfm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\"])\nnum_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\nprint(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\nplot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"max mean v pi\", max(fm_pi_mdp_curated_results[\"Mean V\"]))\nneq = []\nfor i in range(len(fm_vi_mdp.policy)):\n    if fm_vi_mdp.policy[i] != fm_pi_mdp.policy[i]:\n        neq.append(i)\nlen(neq)\nsum(fm_vi_mdp.policy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make(\"FrozenLake-v0\")\nenv.reset()\n#Credit Blake Wang CS7641 @709_f1\nnA, nS = env.nA, env.nS\nP_fl = np.zeros([nA, nS, nS])\nR_fl = np.zeros([nS, nA])\nfor s in range(nS):\n    for a in range(nA):\n        transitions = env.P[s][a]\n        for p_trans, next_s, reward, _ in transitions:\n            P_fl[a,s,next_s] += p_trans\n            R_fl[s,a] = reward\n        P_fl[a,s,:] /= np.sum(P_fl[a,s,:])\n\n\n# frozen_q_policy = policy_iteration(frozen_lake_env, gamma = 0.4)\n# policy_q_score = evaluate_policy(frozen_lake_env, frozen_pi_policy, gamma, n=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P, R = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\nst = time.time()\nfm_q_mdp = hiive.mdptoolbox.mdp.QLearning(P, R, 0.999, epsilon=0, n_iter=10**7, alpha=0.95, skip_check=True)\nfm_q_mdp.run()\nend = time.time()\nend-st\nfm_q_mdp.policy\nfm_q_mdp.epsilon_decay","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fm_q_curated_results = make_time_array(fm_q_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\nplot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"Q-Learning Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"Q-Learning Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"Q-Learning Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fl_q_mdp = hiive.mdptoolbox.mdp.QLearning(P_fl, R_fl, 0.99, epsilon=0.0,epsilon_decay=.95, n_iter=10**7, alpha=0.95, skip_check=True)\nfl_q_mdp.run()\nfm_q_mdp.policy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fl_q_curated_results = make_time_array(fl_q_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\nplot_simple_data(fl_q_curated_results[\"Iteration\"], fl_q_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"Q-Learning Frozen Lake Mean Value over Training\", figure_size=(6,4))\nplot_simple_data(fl_q_curated_results[\"Iteration\"], fl_q_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"Q-Learning Frozen Lake Max Value over Training\", figure_size=(6,4))\nplot_simple_data(fl_q_curated_results[\"Iteration\"], fl_q_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"Q-Learning Frozen Lake Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_args = {\"transitions\":P_fl, \"reward\":R_fl, \"gamma\":0.999,\"epsilon\":10**(-10), \"max_iter\":10**5, \"skip_check\":True}\nfm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_vi_mdp.run()\n#print(fm_vi_mdp)\nfm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\nnum_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\nprint(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\nplot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_args = {\"transitions\":P_fl, \"reward\":R_fl, \"gamma\":0.999,\"epsilon\":10**(-10), \"max_iter\":10**5, \"skip_check\":True}\nfm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\nfm_vi_mdp.run()\n#print(fm_vi_mdp)\nfm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\nnum_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\nprint(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\nplot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Mean V\"], \n                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Max V\"], \n                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\nplot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"times\"], \n                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))\n\n\n# frozen_q_policy = policy_iteration(frozen_lake_env, gamma = 0.4)\n# policy_q_score = evaluate_policy(frozen_lake_env, frozen_pi_policy, gamma, n=1000)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}